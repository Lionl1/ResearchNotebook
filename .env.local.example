# LLM (OpenAI-compatible)
VLLM_API_KEY=sk # API key for the OpenAI-compatible server (set empty to skip auth header)
VLLM_API_BASE=http://localhost:8000/v1 # Base URL for chat/completions
VLLM_MODEL=Qwen/Qwen2.5-7B-Instruct # Default chat model name

# LM Studio (OpenAI-compatible)
# VLLM_API_BASE=http://localhost:1234/v1 # LM Studio default base URL
# VLLM_API_KEY= # Keep empty for LM Studio
# VLLM_MODEL=your-model-name # Model ID shown in LM Studio

# Local embeddings (CPU)
EMBEDDINGS_MODEL=intfloat/multilingual-e5-base # Sentence-transformers model ID
EMBEDDINGS_DEVICE=cpu # cpu or cuda

# Optional: Hugging Face token for private model downloads
HF_TOKEN= # Used to set HUGGING_FACE_HUB_TOKEN

# Disable OCR (extract-text)
OCR_LANGUAGES=none # Tesseract language codes or "none" to disable
ENABLE_PDF_IMAGE_OCR=false # OCR for images embedded in PDFs

# Optional: custom CA bundle for HTTPS scraping/doc downloads (PEM)
SCRAPE_CA_CERT_PATH= # Path to PEM CA bundle
# Optional: requests CA bundle (documents via extract-text)
REQUESTS_CA_BUNDLE= # Path to CA bundle for requests/extract-text

# Embeddings search configuration
CHUNK_SIZE=1500 # Chunk size in characters
CHUNK_OVERLAP=200 # Overlap between chunks
MAX_SOURCE_CHARS=200000 # Cap per source in characters
SEARCH_TOP_K=5 # Default topK for search
CHROMA_DIR=.chroma # Chroma persistence directory
MAX_IMPORT_SIZE_MB=200 # Max import zip size (MB)
MAX_IMPORT_UNPACK_MB=200 # Max unpacked size (MB)
MAX_IMPORT_FILES=4000 # Max files inside import archive

# Local STT
STT_PROVIDER=faster-whisper # Only faster-whisper is supported
STT_MODEL=small # Model name or local path
STT_DEVICE=cpu # cpu or cuda
STT_COMPUTE_TYPE=int8 # Compute type for faster-whisper
STT_BEAM_SIZE=5 # Beam size for decoding

# Optional: Gemini for /api/veo
GEMINI_API_KEY= # API key for Gemini
VEO_MODEL=veo-3.1-generate-preview # Veo model name

# Optional: CORS for external UI
CORS_ORIGINS= # Comma-separated origins (e.g. http://localhost:3000)
